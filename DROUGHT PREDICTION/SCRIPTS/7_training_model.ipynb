{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.0.3-cp38-cp38-win_amd64.whl (10.8 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jean marc\\documents\\master_these\\partie_codes_saison_ii\\version_2024_06_16\\.train\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\jean marc\\documents\\master_these\\partie_codes_saison_ii\\version_2024_06_16\\.train\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jean marc\\documents\\master_these\\partie_codes_saison_ii\\version_2024_06_16\\.train\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, pytz, pandas\n",
      "Successfully installed pandas-2.0.3 pytz-2024.1 tzdata-2024.1\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.0.3-cp38-cp38-win_amd64.whl (10.8 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jean marc\\documents\\master_these\\partie_codes_saison_ii\\version_2024_06_16\\.train\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\jean marc\\documents\\master_these\\partie_codes_saison_ii\\version_2024_06_16\\.train\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jean marc\\documents\\master_these\\partie_codes_saison_ii\\version_2024_06_16\\.train\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, pytz, pandas\n",
      "Successfully installed pandas-2.0.3 pytz-2024.1 tzdata-2024.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\jean marc\\documents\\master_these\\partie_codes_saison_ii\\version_2024_06_16\\.train\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#!pip install scikit-learn tensorflow pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor,GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from config import Config, Constant\n",
    "import os, sys\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultat_compile=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(os.path.join(Config.DATASET_DIR,Config.DATA_1DEK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un répertoire de sortie s'il n'existe pas déjà\n",
    "dir_resultat = Config.RESULT_DIR\n",
    "# Supprimer le dossier existant, y compris son contenu\n",
    "if os.path.exists(dir_resultat):\n",
    "    shutil.rmtree(dir_resultat)\n",
    "# Créer un nouveau répertoire de sortie\n",
    "os.makedirs(dir_resultat, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un répertoire de sortie s'il n'existe pas déjà\n",
    "dir_resultat_sel_var = Config.RESULT_DIR_SEL_VAR\n",
    "# Supprimer le dossier existant, y compris son contenu\n",
    "if os.path.exists(dir_resultat_sel_var):\n",
    "    shutil.rmtree(dir_resultat_sel_var)\n",
    "# Créer un nouveau répertoire de sortie\n",
    "os.makedirs(dir_resultat_sel_var, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation des fonctions utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de la fonction to_numeric_with_nan\n",
    "def to_numeric_with_nan(value):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return pd.NA  # Retourne une valeur manquante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_change(value):\n",
    "    if pd.isna(value):\n",
    "        return pd.NA\n",
    "    elif value == 0.0:\n",
    "        return '0'\n",
    "    elif value == 1.0:\n",
    "        return '1'\n",
    "    else:\n",
    "        return value  # Si la valeur est différente de NaN, 0.0 et 1.0, la renvoyer telle quelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_dek(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Remove rows containing NaN values\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "\n",
    "    # Convert the 'Year' and 'Month' columns to numeric (if necessary)\n",
    "    df[['Year', 'Month','Decade']] = df[['Year', 'Month','Decade']].applymap(to_numeric_with_nan)\n",
    "\n",
    "    # Apply the 'label_change' function to the 'Label Secheresse' column\n",
    "    df['Label Secheresse'] = df['Label Secheresse'].apply(label_change)\n",
    "    # Remove the colonne Date\n",
    "    #df.drop('Date', axis=1, inplace=True)\n",
    "    # Encode categorical columns\n",
    "    ordinal_columns = ['Saison_Pluie','Month','Label Secheresse']\n",
    "    encoder = LabelEncoder()\n",
    "    for col in ordinal_columns:\n",
    "        df[col] = encoder.fit_transform(df[col])\n",
    "    # Shift the 'Label Secheresse' column two months forward for prediction\n",
    "    #df['Secheresse_future'] = df['Label Secheresse']#.shift(2)\n",
    "    # Remove rows containing NaN values again\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "\n",
    "    # Normalize numeric columns that are not in ordinal_columns\n",
    "    #numeric_columns = [col for col in df.columns if col not in ordinal_columns]\n",
    "    numeric_columns = ['v_wind_925','u_wind_850','u_wind_700','u_wind_200','eau_precipitable','t_point_rosee','h_vol_sol_wat','anom_lef_dek','anom_nino_dek']\n",
    "    scaler = StandardScaler()\n",
    "    df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_mon(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Remove rows containing NaN values\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "\n",
    "    # Convert the 'Year' and 'Month' columns to numeric (if necessary)\n",
    "    if 'Decade' in df.columns:\n",
    "        df.drop('Decade', axis=1, inplace=True)\n",
    "    df[['Year', 'Month']] = df[['Year', 'Month']].applymap(to_numeric_with_nan)\n",
    "\n",
    "    # Apply the 'label_change' function to the 'Label Secheresse' column\n",
    "    df['Label Secheresse'] = df['Label Secheresse'].apply(label_change)\n",
    "    # Remove the colonne Date\n",
    "    #df.drop('Date', axis=1, inplace=True)\n",
    "    # Encode categorical columns\n",
    "    ordinal_columns = ['Saison_Pluie','Month','Label Secheresse']\n",
    "    encoder = LabelEncoder()\n",
    "    for col in ordinal_columns:\n",
    "        df[col] = encoder.fit_transform(df[col])\n",
    "    # Shift the 'Label Secheresse' column two months forward for prediction\n",
    "    #df['Secheresse_future'] = df['Label Secheresse']#.shift(2)\n",
    "    # Remove rows containing NaN values again\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "\n",
    "    # Normalize numeric columns that are not in ordinal_columns\n",
    "    #numeric_columns = [col for col in df.columns if col not in ordinal_columns]\n",
    "    numeric_columns = ['v_wind_925','u_wind_850','u_wind_700','u_wind_200','eau_precipitable','t_point_rosee','h_vol_sol_wat','anom_lef_mois','anom_nino_mois']\n",
    "    scaler = StandardScaler()\n",
    "    df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "    #df.drop(['Year','Decade'], axis=1, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['Station','v_wind_925','u_wind_850','u_wind_700','u_wind_200','eau_precipitable','t_point_rosee','h_vol_sol_wat','anom_lef_dek','anom_nino_dek','Saison_Pluie']\n",
    "feature_rnn_dek=['v_wind_925','u_wind_850','u_wind_700','u_wind_200','eau_precipitable','t_point_rosee','h_vol_sol_wat','anom_lef_dek','anom_nino_dek','Saison_Pluie','Label Secheresse']\n",
    "feature_rnn_mon=['v_wind_925','u_wind_850','u_wind_700','u_wind_200','eau_precipitable','t_point_rosee','h_vol_sol_wat','anom_lef_mois','anom_nino_mois','Saison_Pluie','Label Secheresse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model_ranf(df_data,spei,type_sec,localite):\n",
    "    df=df_data.copy()\n",
    "    if 'Decade' in df.columns:\n",
    "        df.drop('Decade', axis=1, inplace=True)\n",
    "    df = df.drop(['Station', 'Year', 'Date'], axis=1)\n",
    "    df['Label Secheresse'] = df['Label Secheresse'].astype(str)\n",
    "\n",
    "    X = df.drop('Label Secheresse', axis=1)\n",
    "    y = df['Label Secheresse']\n",
    "\n",
    "    # Create a pipeline with data preprocessing, feature selection, and classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        #('scaler', StandardScaler()),\n",
    "        ('feature_selector', SelectFromModel(RandomForestClassifier(random_state=42),threshold='median')),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20, 30],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "                               n_jobs=-1, verbose=2)\n",
    "\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(best_model, X, y, cv=k_fold, scoring='accuracy')\n",
    "    mean_accuracy = scores.mean()\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    y_pred = best_model.predict(X)\n",
    "    confusion = confusion_matrix(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, pos_label='1')\n",
    "    recall = recall_score(y, y_pred, pos_label='1')\n",
    "    f1 = f1_score(y, y_pred, pos_label='1')\n",
    "    roc_auc = roc_auc_score(y, y_pred)\n",
    "    best_parameters = grid_search.best_params_\n",
    "\n",
    "    selected_features = X.columns[best_model.named_steps['feature_selector'].get_support()]\n",
    "\n",
    "    resultat_compile[f'Random forest_{localite}_{spei}_{type_sec}']={\n",
    "        #'Selected Features': selected_features,\n",
    "        'Mean Accuracy': round(mean_accuracy,3),\n",
    "        'Precision': round(precision,3),\n",
    "        'Recall': round(recall,3),\n",
    "        'F1 Score': round(f1,3),\n",
    "        'ROC AUC': round(roc_auc,3),\n",
    "        'loss':'-',\n",
    "        'mse':'-',\n",
    "        'mae':'-'\n",
    "    }\n",
    "    \n",
    "    #Metrics dans un fichier\n",
    "    columns = ['Algo', 'Spei', 'Type_Sec', 'Station', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
    "    \n",
    "    # Création du DataFrame avec des colonnes vides\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    metrics_obtenu=['Random forest',spei,type_sec,localite,mean_accuracy, precision,recall,f1,roc_auc]\n",
    "    # Ajout d'une nouvelle ligne au DataFrame\n",
    "    df.loc[len(df)] =metrics_obtenu    \n",
    "    result_file_csv = os.path.join(dir_resultat, f'rf_{spei}_{type_sec}_{localite}.csv')\n",
    "    result_file_excel = os.path.join(dir_resultat, f'rf_{spei}_{type_sec}_{localite}.xlsx')\n",
    "    \n",
    "    df.to_csv(result_file_csv,index=False)\n",
    "    df.to_excel(result_file_excel,index=False)\n",
    "\n",
    "    columns_sel_feat=['Algo', 'Spei', 'Type_Sec', 'Station','Variable_1','Variable_2','Variable_3','Variable_4','Variable_5']\n",
    "    df_sel = pd.DataFrame(columns=columns_sel_feat)\n",
    "    variable1=selected_features[0]\n",
    "    variable2=selected_features[1]\n",
    "    variable3=selected_features[2]\n",
    "    variable4=selected_features[3]\n",
    "    variable5=selected_features[4]\n",
    "    \n",
    "    select_element=['Random forest', spei, type_sec, localite, variable1, variable2, variable3, variable4, variable5]\n",
    "    # Ajout d'une nouvelle ligne au DataFrame\n",
    "    df_sel.loc[len(df_sel)] =select_element   \n",
    "    selected_features_file_csv = os.path.join(dir_resultat_sel_var, f'rf_{spei}_{type_sec}_{localite}.csv')\n",
    "    df_sel.to_csv(selected_features_file_csv, index=False)\n",
    "    selected_features_file_excel = os.path.join(dir_resultat_sel_var, f'rf\n",
    "                                                _{spei}_{type_sec}_{localite}.xlsx')\n",
    "    df_sel.to_excel(selected_features_file_excel, index=False)\n",
    "    \n",
    "    \n",
    "    return best_model,  best_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_boost(df_data,spei, type_sec,localite):\n",
    "    \n",
    "    \n",
    "    df=df_data.copy()\n",
    "    if 'Decade' in df.columns:\n",
    "        df.drop('Decade', axis=1, inplace=True)\n",
    "    df = df.drop(['Station', 'Year', 'Date'], axis=1)\n",
    "    df['Label Secheresse'] = df['Label Secheresse'].astype(str)\n",
    "\n",
    "    X = df.drop('Label Secheresse', axis=1)\n",
    "    y = df['Label Secheresse']\n",
    "\n",
    "    # Create a pipeline with data preprocessing, feature selection, and classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        #('scaler', StandardScaler()),\n",
    "        ('feature_selector', SelectFromModel(GradientBoostingClassifier(random_state=42),threshold='median')),\n",
    "        ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'classifier__max_depth': [3, 4, 5],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "                               n_jobs=-1, verbose=2)\n",
    "\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(best_model, X, y, cv=k_fold, scoring='accuracy')\n",
    "    mean_accuracy = scores.mean()\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    y_pred = best_model.predict(X)\n",
    "    confusion = confusion_matrix(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, pos_label='1')\n",
    "    recall = recall_score(y, y_pred, pos_label='1')\n",
    "    f1 = f1_score(y, y_pred, pos_label='1')\n",
    "    roc_auc = roc_auc_score(y, y_pred)\n",
    "    best_parameters = grid_search.best_params_\n",
    "\n",
    "    selected_features = X.columns[best_model.named_steps['feature_selector'].get_support()]\n",
    "    resultat_compile[f'Gradient boosting_{localite}_{spei}_{type_sec}']={\n",
    "        #'Selected Features': selected_features,\n",
    "        'Mean Accuracy': round(mean_accuracy,3),\n",
    "        'Precision': round(precision,3),\n",
    "        'Recall': round(recall,3),\n",
    "        'F1 Score': round(f1,3),\n",
    "        'ROC AUC': round(roc_auc,3),\n",
    "        'loss':'-',\n",
    "        'mse':'-',\n",
    "        'mae':'-'\n",
    "    }\n",
    "    \n",
    "    columns = ['Algo', 'Spei', 'Type_Sec', 'Station', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
    "    \n",
    "    # Création du DataFrame avec des colonnes vides\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    metrics_obtenu=['Gradient Boost',spei,type_sec,localite,mean_accuracy, precision,recall,f1,roc_auc]\n",
    "    # Ajout d'une nouvelle ligne au DataFrame\n",
    "    df.loc[len(df)] =metrics_obtenu    \n",
    "    result_file_csv = os.path.join(dir_resultat, f'gb_{spei}_{type_sec}_{localite}.csv')\n",
    "    result_file_excel = os.path.join(dir_resultat, f'gb_{spei}_{type_sec}_{localite}.xlsx')\n",
    "    \n",
    "    df.to_csv(result_file_csv,index=False)\n",
    "    df.to_excel(result_file_excel,index=False)\n",
    "    \n",
    "        #Variables selectionnées dans un fichier\n",
    "    #selected_features_df = pd.DataFrame({'Selected Features': selected_features})\n",
    "    columns_sel_feat=['Algo', 'Spei', 'Type_Sec', 'Station','Variable_1','Variable_2','Variable_3','Variable_4','Variable_5']\n",
    "    df_sel = pd.DataFrame(columns=columns_sel_feat)\n",
    "    variable1=selected_features[0]\n",
    "    variable2=selected_features[1]\n",
    "    variable3=selected_features[2]\n",
    "    variable4=selected_features[3]\n",
    "    variable5=selected_features[4]\n",
    "    \n",
    "    select_element=['Gradient Boost', spei, type_sec, localite, variable1, variable2, variable3, variable4, variable5]\n",
    "    df_sel.loc[len(df_sel)] =select_element  \n",
    "    selected_features_file_csv = os.path.join(dir_resultat_sel_var, f'gb_{spei}_{type_sec}_{localite}.csv')\n",
    "    df_sel.to_csv(selected_features_file_csv, index=False)\n",
    "    selected_features_file_excel = os.path.join(dir_resultat_sel_var, f'gb_{spei}_{type_sec}_{localite}.xlsx')\n",
    "    df_sel.to_excel(selected_features_file_excel, index=False)\n",
    "    \n",
    "    \n",
    "    return best_model, best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(X_train, y_train, X_valid, y_valid, X_test, y_test, spei, type_sec, localite):\n",
    "    input_shape = X_train[0].shape  # La forme d'entrée est déterminée par les données d'entraînement\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.LSTM(64, input_shape=input_shape),\n",
    "        keras.layers.Dense(1, activation='sigmoid')  # Couche de sortie avec une seule unité et activation sigmoid pour la classification binaire\n",
    "    ])\n",
    "\n",
    "    # Compilez le modèle avec la perte et l'optimiseur spécifiés\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_valid, y_valid))\n",
    "\n",
    "    # Évaluez le modèle sur les données de test\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)  # Convertir les probabilités en classes binaires (0 ou 1)\n",
    "\n",
    "    # Calculez les métriques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Calculez le Mean Squared Error (MSE) et le Mean Absolute Error (MAE)\n",
    "    #mse = tf.keras.losses.mean_squared_error(y_test, y_pred_proba).numpy()\n",
    "    #mae = tf.keras.losses.mean_absolute_error(y_test, y_pred_proba).numpy()\n",
    "\n",
    "\n",
    "    resultat_compile[f'RNN_{localite}_{spei}_{type_sec}'] = {\n",
    "        'Mean Accuracy': round(np.mean([accuracy]), 3),\n",
    "        'Precision': round(precision, 3),\n",
    "        'Recall': round(recall, 3),\n",
    "        'F1 Score': round(f1, 3),\n",
    "        'ROC AUC': round(roc_auc, 3),\n",
    "        #'loss': round(loss, 3),\n",
    "        #'mse': round(np.mean([mse]), 3),\n",
    "        #'mae': round(np.mean([mae]), 3)\n",
    "    }\n",
    "    columns = ['Algo', 'Spei', 'Type_Sec', 'Station', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
    "\n",
    "    # Création du DataFrame avec des colonnes vides\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    metrics_obtenu = ['RNN', spei, type_sec, localite, np.mean([accuracy]), round(precision, 3), round(recall, 3),\n",
    "                      round(f1, 3), round(roc_auc, 3)]\n",
    "    # Ajout d'une nouvelle ligne au DataFrame\n",
    "    df.loc[len(df)] = metrics_obtenu\n",
    "    result_file_csv = os.path.join(dir_resultat, f'rnn_{spei}_{type_sec}_{localite}.csv')\n",
    "    result_file_excel = os.path.join(dir_resultat, f'rnn_{spei}_{type_sec}_{localite}.xlsx')\n",
    "\n",
    "    df.to_csv(result_file_csv, index=False)\n",
    "    df.to_excel(result_file_excel, index=False)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_rnn(df_data, feature_columns, time_columns, target_column, sequence_length, test_size=0.2, valid_size=0.5):\n",
    "    dataframe=df_data.copy()\n",
    "    # Étape 1 : Triez par la colonne temporelle\n",
    "    #dataframe.sort_values(by=time_columns [ 'Station',time_columns], ascending=[True, True], inplace=True)\n",
    "    if 'Decade' in dataframe.columns:\n",
    "        dataframe.sort_values(by=[ \"Year\", \"Month\", \"Decade\"], ascending=[True, True, True], inplace=True)\n",
    "    else:\n",
    "        dataframe.sort_values(by=[\"Year\", \"Month\"], ascending=[True, True], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Étape 2 : Sélection des caractéristiques et de la cible\n",
    "    features = feature_columns\n",
    "    target = target_column\n",
    "    dataframe[target]=dataframe[target].astype(int)\n",
    "\n",
    "    # Étape 3 : Création de séquences temporelles\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(len(dataframe) - sequence_length):\n",
    "        X.append(dataframe[features].values[i:i+sequence_length])\n",
    "        y.append(dataframe[target].values[i+sequence_length])\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Aplatir les données\n",
    "    X = X.reshape(X.shape[0], -1)  # Aplatissement en 2D\n",
    "\n",
    "    # Étape 4 : Normalisation\n",
    "    scaler = StandardScaler()  #MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Étape 5 : Division des données\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=test_size, shuffle=False)\n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=valid_size, shuffle=False)\n",
    "\n",
    "    # Étape 6 : Préparation des données pour le modèle RNN\n",
    "    X_train = X_train.reshape(X_train.shape[0], sequence_length, len(features))\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], sequence_length, len(features))\n",
    "    X_test = X_test.reshape(X_test.shape[0], sequence_length, len(features))\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTRAINEMENT SUR LES DONNNEES DECADAIRES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_ext_1dek=load_data_dek(str(Config.DATASET_DIR) + '/' + str(Config.DATA_1DEK_EXTR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ext_1dek.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ext_1dek['Label Secheresse'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandonForrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_ext_1dek.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    train_and_evaluate_model_ranf(group_data, '1decade', 'extr', station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_ext_1dek.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    train_and_evaluate_boost(group_data, '1decade', 'extr', station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultat_compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ext_1dek.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_ext_1dek.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    print(f\"Processing station: {station}\")\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_data_for_rnn(group_data, feature_columns=feature_rnn_dek, time_columns=[\"Year\", \"Month\", \"Decade\"], target_column=\"Label Secheresse\", sequence_length=5)\n",
    "    create_rnn_model(X_train, y_train, X_valid, y_valid, X_test, y_test, '1decade', 'extr', station)\n",
    "    print(f\"Processing complete for station: {station}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Severe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sev_1dek=load_data_dek(str(Config.DATASET_DIR) + '/' + str(Config.DATA_1DEK_SEV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandonForrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_sev_1dek.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    train_and_evaluate_model_ranf(group_data, '1decade', 'sev', station)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_sev_1dek.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    train_and_evaluate_boost(group_data, '1decade', 'sev', station)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_sev_1dek.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_data_for_rnn(data_sev_1dek, feature_columns=feature_rnn_dek,time_columns=[\"Year\", \"Month\", \"Decade\"], target_column=\"Label Secheresse\", sequence_length=5)\n",
    "    create_rnn_model(X_train, y_train,X_valid, y_valid,X_test, y_test,'1decade','sev',station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTRAINEMENT SPEI 1-MOIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ext_1mois=load_data_mon(str(Config.DATASET_DIR) + '/' + str(Config.DATA_1MON_EXTR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandonForrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_ext_1mois.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    train_and_evaluate_model_ranf(group_data, '1mois', 'extr', station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_ext_1mois.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    train_and_evaluate_boost(group_data, '1mois', 'extr', station)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_ext_1mois.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_data_for_rnn(data_ext_1mois, feature_columns=feature_rnn_mon,time_columns=[\"Year\", \"Month\"], target_column=\"Label Secheresse\", sequence_length=5)\n",
    "    create_rnn_model(X_train, y_train,X_valid, y_valid,X_test, y_test,'1mois','extr',station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Severe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sev_1mois=load_data_mon(str(Config.DATASET_DIR) + '/' + str(Config.DATA_1MON_SEV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandonForrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_sev_1mois.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    train_and_evaluate_model_ranf(group_data, '1mois', 'sev', station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_sev_1mois.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    train_and_evaluate_boost(group_data, '1mois', 'sev', station)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_sev_1mois.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_data_for_rnn(data_sev_1mois, feature_columns=feature_rnn_mon,time_columns=[\"Year\", \"Month\"], target_column=\"Label Secheresse\", sequence_length=5)\n",
    "    create_rnn_model(X_train, y_train,X_valid, y_valid,X_test, y_test,'1mois','sev',station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTRAINEMENT SPEI 3-MOIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_ext_3mois=load_data_mon(str(Config.DATASET_DIR) + '/' + str(Config.DATA_3MON_EXTR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandonForrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_ext_3mois.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    train_and_evaluate_model_ranf(group_data, '3mois', 'extr', station)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_ext_3mois.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    train_and_evaluate_boost(group_data, '3mois', 'extr', station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_ext_3mois.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_data_for_rnn(data_ext_3mois, feature_columns=feature_rnn_mon,time_columns=[\"Year\", \"Month\"], target_column=\"Label Secheresse\", sequence_length=5)\n",
    "    create_rnn_model(X_train, y_train,X_valid, y_valid,X_test, y_test,'3mois','extr',station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Severe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sev_3mois=load_data_mon(str(Config.DATASET_DIR) + '/' + str(Config.DATA_3MON_SEV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandonForrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_sev_3mois.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    train_and_evaluate_model_ranf(group_data, '3mois', 'sev', station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_sev_3mois.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    train_and_evaluate_boost(group_data, '3mois', 'sev', station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_sev_3mois.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_data_for_rnn(data_sev_3mois, feature_columns=feature_rnn_mon,time_columns=[\"Year\", \"Month\"], target_column=\"Label Secheresse\", sequence_length=5)\n",
    "    create_rnn_model(X_train, y_train,X_valid, y_valid,X_test, y_test,'3mois','sev',station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTRAINEMENT SPEI 6-MOIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_ext_6mois=load_data_mon(str(Config.DATASET_DIR) + '/' + str(Config.DATA_6MON_EXTR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ext_6mois.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandonForrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_ext_6mois.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    train_and_evaluate_model_ranf(group_data, '6mois', 'extr', station)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_ext_6mois.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    train_and_evaluate_boost(group_data, '6mois', 'extr', station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ext_6mois.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_ext_6mois.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_data_for_rnn(data_ext_6mois, feature_columns=feature_rnn_mon,time_columns=[\"Year\", \"Month\"], target_column=\"Label Secheresse\", sequence_length=5)\n",
    "    create_rnn_model(X_train, y_train,X_valid, y_valid,X_test, y_test,'6mois','extr',station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Severe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sev_6mois=load_data_mon(str(Config.DATASET_DIR) + '/' + str(Config.DATA_6MON_SEV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandonForrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rand_sev_6mois=train_and_evaluate_model_ranf(data_sev_6mois)\n",
    "grouped_data = data_sev_6mois.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    train_and_evaluate_model_ranf(group_data, '6mois', 'sev', station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_sev_6mois.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    train_and_evaluate_boost(group_data, '6mois', 'sev', station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data_sev_6mois.groupby('Station')\n",
    "\n",
    "# Parcourez chaque groupe et appliquez la fonction\n",
    "for station, group_data in grouped_data:\n",
    "    #data_by_st = group_data.drop(['station'], axis=1)  # Assurez-vous de supprimer la colonne 'station' si elle ne fait pas partie de vos fonctionnalités\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_data_for_rnn(data_sev_6mois, feature_columns=feature_rnn_mon,time_columns=[\"Year\", \"Month\"], target_column=\"Label Secheresse\", sequence_length=5)\n",
    "    create_rnn_model(X_train, y_train,X_valid, y_valid,X_test, y_test,'6mois','sev',station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTRAINEMENT SPEI 1-MOIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultat_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame(resultat_compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('./resultat.csv')\n",
    "result.to_excel('./resultat.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
